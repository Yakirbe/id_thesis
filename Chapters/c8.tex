% Chapter 1

\chapter{Experiments} % Main chapter title

\label{Chapter8}

In this chapter we display a novel usage of our new the $ID$ attributes and qualities.

Here we will use our $ID$ method in order to obtain a perceptual colors distance metric learning, which is a high demanded function in computer vision region. Our examination is based and referring to \cite{perp_color}, which developed a local metric for this problem. 

This set of experiments tries to adapt a perceptual color difference metrics, based on various color samples originated from several cameras, angles, illuminations etc.
\\

Our $ID$ method would be examined in this section solely by the original experiment's accuracy measures - Mean Absolute Error \cite{MAE} and STRESS \cite{STRESS}
\\
Further discussion will refer to the various exams may apply on our $ID$ method according to our described theory.

\section{Experiment Procedure}



\textbf{object pairs ID embedding} \ref{Chapter3} will be demonstrated by applying its embedding method over pairs which was taken and labeled from the Farnsworth-Munsell 100 hue-test set \cite{furnsworth}.
\subsection{Dataset}

Dataset of this experiment is assembled from single color patches, which displayed in a set of images, where each image was genarated by a specific set of image features, such illumination, camera type (4 different cameras are involved in the dataset creation) , lance type , background etc.

\begin{figure}[h] \label{set_ds}
			\includegraphics[width=\linewidth,height=12cm,keepaspectratio]{Figures/set_ds}
			\caption[set ds]
			{set ds}
\end{figure}

For each color patch, a $L^*A^*B^*$ \cite{lab} coordinates are given. 
Since the pairs of patches should be close by their CIELAB \cite{CIELAB} coordinates in order to adapt a good color difference assessment, a set in the final dataset is only a set of patches, where its CIELAB euclidean distance is relatively close, i.e. $\Delta E \leq 5$ .


\begin{figure}[h] \label{patch_positions}
			\includegraphics[width=\linewidth,height=12cm,keepaspectratio]{Figures/patch_positions}
			\caption[patches order]
			{patches order}
\end{figure}


\subsection{Models to train}
Then, $ID$ learning method will try to learn some similarity models according to a couple of test conditions:

\begin{itemize}
	\item \textbf{unseen colors} - dividing the entire patches set into train-test sets
	\item \textbf{unseen cameras} - assigning 3 out of 4 cameras sets as train set, and make the remained camera set as test set. \\
		training set cameras:
		\begin{itemize}
			\item Kodak DCS	Pro 14n
			\item Konica Minolta Dimage Z3
			\item Nikon Coolpix S6150
		\end{itemize}	
		
		test set camera:	
		\begin{itemize}
			\item Sony DCR-SR32
		\end{itemize}
		
\end{itemize}

\subsection{Interpolation}
Data dimension is of course $n=3$ (for lab/RGB \cite{RGB} coordinates), and for an object pair the dimension is $2n=6$.
As described in \ref{Chapter3}, interpolation is performed dimension-wise along the training set, after selecting data center per dimension by manual/automatic method.
\\
In our experiment we have selected an automatic cross validation \cite{cross validation} method for assessing the optimal number of centers per dimension, and find those by using k-means \cite{kmeans} algorithm.
\\
For this number of extracted centers we add 2 extreme points for applying proper interpolation for data values beyond limits of the discovered centers.
\\
Now that we have found data centers for each dimension we apply our interpolation method \ref{interpolation pairs} on our data (on both train-test sets) and extract a $\overrightarrow{a} \in \Re^6$ coefficient vector per color patches pair, which is ready to be embedded.


\subsection{Embedding}
Embedding is performed by applying an embedding of the coefficient vectors on a sparse form such $\overrightarrow{e} \in \Re^{\prod_{i=1}^{2n}{length(\overrightarrow{c_i})}}$ , where $n = 3$.
Each coefficient vector element is assigned to the exact simplex index in the $\overrightarrow{e}$ embedded vector.
\\
For memory savings, we have performed a reduction to all columns along the entire dataset, where all elements were equal to zero.
Since our embedded vectors are sparsed, it was quite common to have over $90 \% $ of the dimensions to be total zeroed so final actual dimension of our sets were almost always $ \le 100$.

\subsection{Learning}
Learning is performed as a linear regression model training.
We learn by using SGD \cite{SGD} learning algorithm, and with a MSE \cite{MSE} loss function.
We also applied a standard Tichonov \cite{Tichonov} regularization on the learning process as described in the learning chapter \ref{Chapter6}.






\section{Results}

\begin{figure}[h] \label{original paper results}
			\includegraphics[width=\linewidth,height=12cm,keepaspectratio]{Figures/original_paper_results}
			\caption[orig res]
			{Generalization of the learned metrics to new colors; (b) Generalization of
			the learned metrics to new cameras. For (a) and (b), we plotted the Mean and STRESS
			values as a function of the number of clusters. The horizontal dashed line represents
			the STRESS baseline of $\Delta E_{00}$ . For the sake of readability, we have not plotted the
			mean baseline of $\Delta E_{00}$ at 1.70.}
\end{figure}

